<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Terminal</title>
    <link>/tags/ai/</link>
    <description>Recent content in AI on Terminal</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 09 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>abhiraj-blog-3</title>
      <link>/posts/mac-mini-vs-diy-ai-rig--best-local-llm-setup-for-broke-devs/</link>
      <pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate>
      <guid>/posts/mac-mini-vs-diy-ai-rig--best-local-llm-setup-for-broke-devs/</guid>
      <description>&lt;h2 id=&#34;-mac-mini-for-local-ai-thought-about-it&#34;&gt;üß† Mac Mini for Local AI? Thought About It&amp;hellip;&lt;/h2&gt;&#xA;&lt;p&gt;Lately, I‚Äôve seen a lot of people flexing their &lt;strong&gt;Mac Mini stacks&lt;/strong&gt; for local AI/LLM workloads. Seemed cool, minimal, power-efficient. So I thought:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;‚ÄúWhy not grab a second-hand Mac Mini and connect it to my homelab? Just for local AI stuff?‚Äù&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Well&amp;hellip; turns out there‚Äôs a catch.&#xA;&lt;img src=&#34;/images/Pasted%20image%2020250709182344.png&#34; alt=&#34;Pasted image 20250709182344.png&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;-the-problem-with-cheap-mac-minis&#34;&gt;üöß The Problem with Cheap Mac Minis&lt;/h2&gt;&#xA;&lt;p&gt;Most second-hand Mac Minis come with &lt;strong&gt;8GB or 16GB of Unified Memory&lt;/strong&gt;, and yeah, macOS can allocate up to &lt;strong&gt;75% of that to GPU compute&lt;/strong&gt;. But if you‚Äôre trying to run models like:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
